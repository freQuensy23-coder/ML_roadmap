По умолчанию нам необходимо для каждого токена расчитать аттеншн с каждым другим токеном - то есть асимптотика $N^2$.  Если у нас m слоев декодера, в каждом k слоев attention суммарная сложность выходит $O(N^2\cdot k \cdot m)$ . Если мы спускаемся еще ниже, можно домножить на размерность латентного пространства в степени сложности алгоритма умножения матриц, реализованом на вашем ускорителе (обычно что то порядка 2.5). Однако в современных LLM применяют различным методы, позволяющее ускорить процесс вычисления attention, там более оптимальная асимптотика.

В RNN нам нужно вычислить латентное представление каждого токена на каждом токене - значит у нас линейная сложность по числу токенов.
