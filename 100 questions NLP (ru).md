# 100 questions about NLP

В создании списка приняли участие
[ds girl](https://t.me/girlinds)
[Alexander Babiy](https://www.linkedin.com/in/asbabii/)
[канал что-то на DL-ском](https://t.me/nadlskom)
[канал Dealer.AI](https://t.me/dealerAI)
[канал алиса олеговна](https://t.me/alisaolega)
[канал Плюшевый Питон](https://t.me/plush_python)
[ML phys](https://t.me/mlphys)

# **CLASSIC NLP**

* **TF-IDF  & ML (8)**
  * [ ] 
    1. Напишите **TF-IDF** с нуля?
  * [ ] 
    2. Что такое нормализация в **TF-IDF**?
  * [ ] 
    3. Зачем вы вообще знаете про **TF-IDF** в наше время и как можете использовать в сложных моделях?
  * [ ] 
    4. Объясните, как работает Наивный Баес? Для чего вы можете его использовать?
  * [ ] 
    5. Как может переобучиться SVM?
  * [ ] 
    6. Объясните возможные методы предобработки текста (**лемматизацию** и **стемминг**). Какие алгоритмы для этого знаете и в каких случаях будете использовать?
  * [ ] 
    7. Какие метрики для близости текстов вы знаете?
  * [ ] 
    8. Объясните разницу между **косинусной** близостью и косинусным расстоянием. Какое из этих значений может быть негативным? Как вы будете их использовать?
* **METRICS (7)**
  * [ ] 
    9. Объясните precision и recall разницу простыми словами и на что вы будете смотреть при отсутствии f1 score?
  * [ ] 
    10. В каком случае вы будете наблюдать изменение **specificity**?
  * [ ] 
    11. Когда вы будете смотреть на macro, а когда на micro метрики? Почему существует weighted метрика?
  * [ ] 
    12. Что такое perplexity? С чем мы можем ее считать?
  * [ ] 
    13. Что такое метрика **BLEU**?
  * [ ] 
    14. Объясните разницу между разными видами **ROUGE** метрики?
  * [ ] 
    15. В чем отличие BLUE от ROUGE?
* **WORD2VEC(9)**
  * [ ] 
    16. Объясните как учится **Word2Vec**? Какая функция потерь? Что максимизируется?
  * [ ] 
    17. Какие способы получения эмбеддингов знаете? Когда какие будут лучше?
  * [ ] 
    18. В чем отличие между static и contextual эмбеддингов?
  * [ ] 
    19. Какие две основные архитектуры вы знаете и какая из них учится быстрее?
  * [ ] 
    20. В чем разница между Glove, ELMO, FastText и **Word2Vec**?
  * [ ] 
    21. Что такое negative sampling и зачем он нужен? Какие еще трюки у word2vec знаете и как можете применять у себя?
  * [ ] 
    22. Что такое dense и sparse эмбеддинги? Приведите примеры.
  * [ ] 
    23. Почему может быть важна размерность эмбеддинга?
  * [ ] 
    24. Какие проблемы могут возникнуть при обучении **Word2Vec** на коротких текстовых данных, и как можно с ними справиться?
* **RNN & CNN(7)**
  * [ ] 
    25. Сколько обучающих параметров в простой 1 слойной **RNN**?
  * [ ] 
    26. Как обучается RNN?
  * [ ] 
    27. Какие проблемы есть в RNN?
  * [ ] 
    28. Какие виды RNN сетей вы знаете? Объясните разницу между **GRU** и LSTM?
  * [ ] 
    29. Какие параметры мы можем тюнить в такого вида сетях? (Стакать, кол-о слоев)
  * [ ] 
    30. Что такое затухающие градиенты для RNN? И как вы решаете эту проблему?
  * [ ] 
    31. Зачем в NLP Convolutional neural network и как вы его можете использовать? С чем вы можете сравнить cnn в рамках парадигмы attention?

## **NLP and TRANSFORMERS**

* **ATTENTION AND TRANSFORMER ARCHITECTURE (15)**
  
  * [ ] 
    32. \[\[Как считаете **attention**?\]\]
  * [ ] 
    33. [Сложность attention? Сравните с сложностью в rnn?](%D0%A1%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8C%20attention%3F%20%D0%A1%D1%80%D0%B0%D0%B2%D0%BD%D0%B8%D1%82%D0%B5%20%D1%81%20%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8C%D1%8E%20%D0%B2%20rnn%3F.md)
  * [ ] 
    34. \[\[Сравните RNN и **attention**? В каком случае будете использовать attention, а когда RNN?\]\]
  * [ ] 
    35. Напишите attention с нуля.
  * [ ] 
    36. [Объясните маскирование в attention.](%D0%9E%D0%B1%D1%8A%D1%8F%D1%81%D0%BD%D0%B8%D1%82%D0%B5%20%D0%BC%D0%B0%D1%81%D0%BA%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%B2%20attention..md)
  * [ ] 
    37. Какая размерность у матриц self-attention?
  * [ ] 
    38. [В чем разница между BERT и GPT в рамках подсчета attention?](%D0%92%20%D1%87%D0%B5%D0%BC%20%D1%80%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0%20%D0%BC%D0%B5%D0%B6%D0%B4%D1%83%20BERT%20%D0%B8%20GPT%20%D0%B2%20%D1%80%D0%B0%D0%BC%D0%BA%D0%B0%D1%85%20%D0%BF%D0%BE%D0%B4%D1%81%D1%87%D0%B5%D1%82%D0%B0%20attention%3F.md)
  * [ ] 
    39. Какая размерность у эмбединового слоя в трансформере?
  * [ ] 
    40. Почему эмбеддинги называются контекстуальными? Как это работает?
  * [ ] 
    41. Что используется в трансформере **layer norm** или **batch norm** и почему?
  * [ ] 
    42. Зачем в трансформерах **PreNorm** и **PostNorm**?
  * [ ] 
    43. Объясните разницу между **soft** и **hard** (local/global) attention?
  * [ ] 
    44. Объясните multihead attention.
  * [ ] 
    45. Какие другие виды механизмов внимания вы знаете? На что направлены эти модификации?
  * [ ] 
    46. На сколько усложнится self-attention при увеличении числа голов? 
* **TRANSFORMER MODEL TYPES (7)**
  
  * [ ] 
    47. Почему BERT во многом проигрывает **RoBERTa** и что вы можете взять у RoBERTa?
  * [ ] 
    48. Что такое **T5** и **BART** модели? Чем они отличаются?
  * [ ] 
    49. Что такое **task-agnostic** модели? Приведите примеры.
  * [ ] 
    50. Объясните transformer модели сравнивая BERT, GPT и T5.
  * [ ] 
    51. Какая большая проблема есть в моделях BERT, GPT и тд относительно знаний модели? Как это можно решать?
  * [ ] 
    52. Как работает decoder like а-ля GPT на обучении и инференсе. В чем разница?
  * [ ] 
    53. Объясните разницу между головами и слоями в трансформер моделях.
* **POSITIONAL ENCODING (6)**
  
  * [ ] 
    54. Почему в эмбеддингах transformer моделей с attention теряется информациях о позициях?
  * [ ] 
    55. Объясните подходы для позициональных эмбеддингов и их плюсы и минусы.
  * [ ] 
    56. Почему нельзя просто добавить эмбеддинг с индексом токена?
  * [ ] 
    57. Почему мы не учим positional embeddings?
  * [ ] 
    58. Что такое **relative** и **absolute** positional encoding?
  * [ ] 
    59. Подробно объясните принцип работы **rotary** positional embeddings
* **PRETRAINING (4)**
  
  * [ ] 
    60. Как обучается **causal language modelling**?
  * [ ] 
    61. Когда мы используем предобученную модель?
  * [ ] 
    62. Как обучить transformer с нуля? Объясните свой пайплайн и в каком случае вы будете этим заниматься.
  * [ ] 
    63. Какие модели кроме BERT и GPT по различным задачам предобучения вы знаете?
* **TOKENIZERS (9)**
  
  * [ ] 
    64. Какие виды **токенайзеров** вы знаете? Сравните их.
  * [ ] 
    65. Можете ли вы расширять токенайзер? Если да, то в каком случае вы будете этим заниматься? Когда вы будете переобучать токенайзер? Что необходимо сделать при добавлении новых токенов?
  * [ ] 
    66. Чем обычные токены отличаются от **специальных** токенов?
  * [ ] 
    67. Почему в трансформерах не используется лемматизация? И зачем нам нужны токены?
  * [ ] 
    68. Как обучается токенизатор? Объясните на примерах **WordPiece** и **BPE**.
  * [ ] 
    69. На какой позиции стоит CLS вектор? Почему?
  * [ ] 
    70. Какой токенизатор используется в BERT, а какой в GPT?
  * [ ] 
    71. Объясните как современные токенизаторы обрабатывают out-of-vocabulary words?
  * [ ] 
    72. На что влияет tokenizer vocab size? Как вы будете его выбирать в случае нового обучения?
* **TRAINING (14)**
  
  * [ ] 
    73. Что такое дисбаланс классов? Как это можно увидеть? Назовите все подходы для решения этой проблемы.
  * [ ] 
    74. Можно ли использовать на инференсе dropout и почему?
  * [ ] 
    75. В чем отличие оптимизатора **Adam** от AdamW?
  * [ ] 
    76. Как изменяться потребляемые ресурсы при **gradient accumulation**?
  * [ ] 
    77. Как оптимизировать потребление ресурсов при обучении?
  * [ ] 
    78. Какие знаете способы распределенного обучения?
  * [ ] 
    79. [Что такое текстовые аугментации? Назовите все методы, что знаете.](%D0%A7%D1%82%D0%BE%20%D1%82%D0%B0%D0%BA%D0%BE%D0%B5%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D0%B5%20%D0%B0%D1%83%D0%B3%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D0%B8%3F%20%D0%9D%D0%B0%D0%B7%D0%BE%D0%B2%D0%B8%D1%82%D0%B5%20%D0%B2%D1%81%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B,%20%D1%87%D1%82%D0%BE%20%D0%B7%D0%BD%D0%B0%D0%B5%D1%82%D0%B5..md)
  * [ ] 
    80. Почему стал реже использовать паддинг? Что делают вместо этого?
  * [ ] 
    81. Объясните как работает **warm-up**?
  * [ ] 
    82. Объясните концепцию **gradient clipping**?
  * [ ] 
    83. Как работает **teacher forcing**, приведите примеры?
  * [ ] 
    84. Как и почему нужно использовать **skip connection**?
  * [ ] 
    85. Что такое адаптеры? Где и как мы можем их использовать?
  * [ ] 
    86. \[\[Объясните концепции **metric learning**. Какие подходы вам известны?\]\]
* **INFERENCE (4)**
  
  * [ ] 
    87. За что отвечает температура в softmax? Какую вы будете выставлять?
  * [ ] 
    88. Объясните виды **sampling** при генерации? **top-k**, top-p, **nucleus sampling**?
  * [ ] 
    89. Какая сложность у **beamsearch** и как он работает?
  * [ ] 
    90. Что такое sentence embedding? Какими способами вы можете его получить?
* **LLM (10)**
  
  * [ ] 
    91. [Как работает LoRA? Как вы будете выбирать параметры? Представьте, что мы хотим дообучить большую языковую модель, делаем LORA с маленьким R, но модель все равно не лезет по памяти, что еще можно сделать?](%D0%9A%D0%B0%D0%BA%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82%20LoRA%3F%20%D0%9A%D0%B0%D0%BA%20%D0%B2%D1%8B%20%D0%B1%D1%83%D0%B4%D0%B5%D1%82%D0%B5%20%D0%B2%D1%8B%D0%B1%D0%B8%D1%80%D0%B0%D1%82%D1%8C%20%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D1%8B%3F%20%D0%9F%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D1%8C%D1%82%D0%B5,%20%D1%87%D1%82%D0%BE%20%D0%BC%D1%8B%20%D1%85%D0%BE%D1%82%D0%B8%D0%BC%20%D0%B4%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B8%D1%82%D1%8C%20%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D1%83%D1%8E%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D1%83%D1%8E%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C,%20%D0%B4%D0%B5%D0%BB%D0%B0%D0%B5%D0%BC%20LORA%20%D1%81%20%D0%BC%D0%B0%D0%BB%D0%B5%D0%BD%D1%8C%D0%BA%D0%B8%D0%BC%20R,%20%D0%BD%D0%BE%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%B2%D1%81%D0%B5%20%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%20%D0%BD%D0%B5%20%D0%BB%D0%B5%D0%B7%D0%B5%D1%82%20%D0%BF%D0%BE%20%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D0%B8,%20%D1%87%D1%82%D0%BE%20%D0%B5%D1%89%D0%B5%20%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%20%D1%81%D0%B4%D0%B5%D0%BB%D0%B0%D1%82%D1%8C%3F.md)
  * [ ] 
    92. \[\[В чем отличие **prefix tuning** от **p-tuning** и от **prompt tuning**?\]\]
  * [ ] 
    93. [Объясните scaling law.](%D0%9E%D0%B1%D1%8A%D1%8F%D1%81%D0%BD%D0%B8%D1%82%D0%B5%20scaling%20law..md)
  * [ ] 
    94. [Объясните все этапы обучения LLM. От какого из этапов мы можем отказывать и в каких случаях.](%D0%9E%D0%B1%D1%8A%D1%8F%D1%81%D0%BD%D0%B8%D1%82%D0%B5%20%D0%B2%D1%81%D0%B5%20%D1%8D%D1%82%D0%B0%D0%BF%D1%8B%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F%20LLM.%20%D0%9E%D1%82%20%D0%BA%D0%B0%D0%BA%D0%BE%D0%B3%D0%BE%20%D0%B8%D0%B7%20%D1%8D%D1%82%D0%B0%D0%BF%D0%BE%D0%B2%20%D0%BC%D1%8B%20%D0%BC%D0%BE%D0%B6%D0%B5%D0%BC%20%D0%BE%D1%82%D0%BA%D0%B0%D0%B7%D1%8B%D0%B2%D0%B0%D1%82%D1%8C%20%D0%B8%20%D0%B2%20%D0%BA%D0%B0%D0%BA%D0%B8%D1%85%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D1%8F%D1%85..md)
  * [ ] 
    95. \[\[Как работает **RAG**? Чем он отличается от few-shot KNN?\]\]
  * [ ] 
    96. Какие методы квантизации вы знаете? Можем ли мы тюнить квантизованные модели?
  * [ ] 
    97. Как вы сможете предотвращать катастрофическое забывание у LLM?
  * [ ] 
    98. \[\[Объясните принцип работы **KV cache**, **Grouped-Query Attention** и **MultiQuery Attention**.\]\]
  * [ ] 
    99. Объясните технологию, которая стоит за MixTral, в чем ее плюсы и минусы?
  * [ ] 
    100. [Че как сам? Как дела?](%D0%A7%D0%B5%20%D0%BA%D0%B0%D0%BA%20%D1%81%D0%B0%D0%BC%3F%20%D0%9A%D0%B0%D0%BA%20%D0%B4%D0%B5%D0%BB%D0%B0%3F.md)
* **ANALYSE questions  дополнительные**
  
  1. Назовите наиболее важные на ваш взгляд современные тренды в NLP.
  1. Генеративная decoder-only модель выучила некоторые "плохие" паттерны в данных. В результате иногда она генерирует "плохие" тексты. Как сделать так, чтобы она их не генерировала? (Вопрос намеренно поставлен широко. От стажёра/джуна 1-2 гипотезы, от мидла - 2-3 возможных подхода, от сеньора - 4-5 подходов с разбором их достоинств и недостатков)
  1. Допустим нам надо обучить классификатор на k классов, из разметки есть по 10 примеров на каждый, как учить? 
     Предполагается, что либо больше про синтетические данные кандидат расскажет, как их генерить и тд, либо в идеальном случае про few-shot learning.
  1. [Может ли быть такое, что модель дает вероятность какого-то класса 90+%, но при этом все равно ошибается?](%D0%9C%D0%BE%D0%B6%D0%B5%D1%82%20%D0%BB%D0%B8%20%D0%B1%D1%8B%D1%82%D1%8C%20%D1%82%D0%B0%D0%BA%D0%BE%D0%B5,%20%D1%87%D1%82%D0%BE%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%B4%D0%B0%D0%B5%D1%82%20%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C%20%D0%BA%D0%B0%D0%BA%D0%BE%D0%B3%D0%BE-%D1%82%D0%BE%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B0%2090+%25,%20%D0%BD%D0%BE%20%D0%BF%D1%80%D0%B8%20%D1%8D%D1%82%D0%BE%D0%BC%20%D0%B2%D1%81%D0%B5%20%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%20%D0%BE%D1%88%D0%B8%D0%B1%D0%B0%D0%B5%D1%82%D1%81%D1%8F%3F.md)
  1. Назовите последнюю статью, которую вы читали?
