1. Представим LLM работающую в режиме Causal LM. Мы прогоняем текст через трансформер, LM Head из латентного представления достает логиты следующего токена. Потом весь этот процесс повторяется, мы заново считаем атеншн между исходным текстом + новым токеном и генерируем следующий. Простыми словами, представьте что при написании текста, перед тем как вы пишите каждое следующее слово вы заново читаете и осознает прошлый текст, понимаете его, пишите слово, и теперь нуля читаем новый текст + ваше одно слово. Понятно что тут закралась некоторая неэффективность, которую как раз решает кэш. Вы можете "запомнить" все слова в контексте а так же отоношения между ними, и не рассчитывать это все на каждом следующем токене.
1. Для повышения эффективности работы модели можно оптимизировать процесс вычисления Values и Keys в Attention Layer, используя только одну голову или часть голов. При этом, благодаря независимому вычислению Queries различными головами, модель сохраняет способность анализировать разнообразные аспекты текста. Этот подход позволяет ускорить обработку данных, сократив объем необходимых вычислений, в то же время сохраняя многоуровневый анализ, который является ключевым для понимания и интерпретации естественного языка.
   ![Pasted image 20231218153146.png](Pasted%20image%2020231218153146.png)
