Рассмотрим для примера обучение Encoder-Decoder Transformer для перевода. Attention слои в Decoder -е могут "смотреть" в исходный текст (в Encoder) а так же в те токены перевода которые были до фиксированного. То есть в тексте "You are engineer" токен are может смотреть только на себя или на you, но не на engineer, 
