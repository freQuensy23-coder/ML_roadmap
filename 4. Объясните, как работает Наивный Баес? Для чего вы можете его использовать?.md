Наивный Байесовский классификатор (Naive Bayes classifier) — это вероятностный алгоритм машинного обучения, основанный на применении теоремы Байеса с "наивным" допущением о независимости признаков. Несмотря на название, этот метод часто показывает хорошие результаты в задачах классификации, особенно когда число признаков велико.

### Как работает Наивный Байес:

1. **Теорема Байеса**:
   Наивный Байесовский классификатор основан на теореме Байеса, которая гласит:

   $$
   P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
   $$

   Где:
   - \( P(C|X) \) — вероятность того, что объект принадлежит классу \( C \) при условии, что наблюдается \( X \) (постериорная вероятность).
   - \( P(X|C) \) — вероятность наблюдения признаков \( X \) при условии, что объект принадлежит классу \( C \) (правдоподобие).
   - \( P(C) \) — вероятность того, что объект принадлежит классу \( C \) без учета признаков (априорная вероятность).
   - \( P(X) \) — вероятность наблюдения признаков \( X \) (нормализующая константа).

2. **Наивное допущение**:
   Алгоритм делает "наивное" допущение о том, что все признаки \( X_i \) независимы друг от друга при условии выбранного класса \( C \):

   $$
   P(X|C) = P(X_1|C) \cdot P(X_2|C) \cdot \ldots \cdot P(X_n|C)
   $$

   Это существенно упрощает вычисления, поскольку вместо работы с многомерными вероятностями достаточно рассчитать произведение одномерных вероятностей.

3. **Выбор класса**:
   Для классификации нового объекта алгоритм вычисляет апостериорные вероятности для каждого класса и выбирает тот, для которого вероятность максимальна:

   $$
   C_{\text{best}} = \arg\max_{C} P(C|X)
   $$

### Преимущества и недостатки:

**Преимущества**:
- Простота реализации и вычислительная эффективность.
- Хорошие результаты на задачах с большим числом признаков.
- Может использоваться даже с небольшим количеством данных для обучения.

**Недостатки**:
- Основное допущение о независимости признаков часто не выполняется, что может приводить к снижению точности.
- Модель чувствительна к несбалансированным данным (если классы неравномерно распределены).

### Примеры
Можно использовать для классификации спама, где слова будут независимыми фичами
