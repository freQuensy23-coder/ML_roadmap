1. В архитектуре трансформеров, механизм "внимания"
   (Attention) играет центральную роль в понимании и
   обработке естественного языка. Эта подход
   позволяет модели сосредоточиваться на ключевых
   частях текста, игнорируя менее значимые детали,
   аналогично тому, как человек читает и понимает
   язык, обращая внимание на важные слова и фразы.
   Этот процесс включает вычисление трех
   ключевых компонентов для каждого латентного представления токена: Value (значимость токена), Key и Query.
   Скалярное произведение Key и Query для разных
   токенов определяет их взаимную значимость, что
   помогает модели оценивать важность и связь
   элементов текста.
   Таким образом выход Attention слоя это:
   $$Attention(Q, K, V) = softmax(\frac{Q\cdot K^T}{\sqrt{d}})V$$

2.Впервые механизм attention предложили вот тут [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) - для задачи машинного перевода в 2014 году. Основной проблемой систем перевода основанного на рекурентах  (SoTA на тот момент) - отрицательная кореляция качества перевода от длины, или простыми словами RNN хуже работала на длинных текстах, так как забывала что было в начале фразы, пока доходила до конца. 

Популярность обрел после статьи
гугла [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - там ее применяли для Encoder-Decoder LLM
