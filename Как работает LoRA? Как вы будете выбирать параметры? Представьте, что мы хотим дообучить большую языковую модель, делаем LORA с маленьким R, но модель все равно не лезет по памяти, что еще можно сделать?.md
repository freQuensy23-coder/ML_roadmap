Идея LoRA что мы добавляем к матрица весов нейросети низкоранговые адаптеры, подробнее про это [тут](https://habr.com/ru/articles/747534/).

1. Применения лора адаптеров не ко всем слоям а только к последним N
1. Кванторизация исходной модели (QLoRA)
1. Более эффективные оптимизаторы, adamW-8bit, adamW-fused, adafactor итп
1. Другие методы обучения - ptuning итп - для больших моделей они показывают приемлимое качество, сравнимое с LoRA
1. Для маленьких моделей из-за того что peft написано довольно криво, дообучать полную модель бывает эффективнее по памяти чем LoRA адаптеры))
